# Технические Спецификации: Sitemap Extract

## 1. Обзор Проекта

### 1.1 Назначение
Приложение предназначено для эффективной обработки XML-карт сайтов (sitemaps) и извлечения URL-адресов. Система поддерживает множественные источники входных данных, обработку вложенных карт сайтов неограниченной глубины и использует многопоточность для оптимизации производительности.

### 1.2 Область Применения
- Извлечение URL из XML-карт сайтов различных форматов
- Обработка больших объемов данных (сотни вложенных карт, миллионы URL)
- Обход антибот-систем при загрузке карт сайтов
- Автоматизированный сбор структуры сайтов

### 1.3 Ключевые Характеристики
- Поддержка обработки сотен вложенных карт сайтов
- Обработка миллионов URL
- Многопоточная архитектура для высокой производительности
- Защита от антибот-систем
- Детальное логирование всех операций

---

## 2. Архитектура Системы

### 2.1 Архитектурный Паттерн
Система реализована по модульной архитектуре с разделением ответственности:
- Модуль сетевого взаимодействия
- Модуль парсинга XML
- Модуль управления очередями
- Модуль файловых операций
- Модуль логирования

### 2.2 Модель Обработки Данных
Система использует **очередь с динамическим расширением** (queue-based processing):
1. Инициализация очереди стартовыми URL
2. Последовательное извлечение URL из очереди
3. Параллельная обработка через пул потоков
4. Добавление обнаруженных вложенных карт в очередь
5. Продолжение обработки до опустошения очереди

### 2.3 Модель Конкурентности
- **ThreadPoolExecutor** с фиксированным количеством рабочих потоков (5 потоков)
- Использование Future-объектов для асинхронной обработки
- Thread-safe операции через множества (set) для отслеживания обработанных URL

---

## 3. Технологический Стек

### 3.1 Язык Программирования
- **Python 3.x** (минимальная версия не специфицирована, рекомендуется 3.7+)

### 3.2 Основные Зависимости

#### 3.2.1 Cloudscraper (версия 1.2.58)
**Назначение**: Обход антибот-систем (Cloudflare, etc.)
**Функциональность**:
- Автоматическое решение JavaScript-челленджей
- Эмуляция браузерного поведения
- Обработка cookies и сессий

#### 3.2.2 Argparse (версия 1.4.0)
**Назначение**: Парсинг аргументов командной строки
**Функциональность**:
- Обработка позиционных и опциональных аргументов
- Автоматическая генерация справки
- Валидация входных параметров

### 3.3 Стандартные Библиотеки Python

#### xml.etree.ElementTree
- Парсинг XML-документов
- Обработка пространств имен XML
- Навигация по DOM-дереву

#### gzip
- Декомпрессия .gz архивов
- Потоковое чтение сжатых данных

#### concurrent.futures
- Управление пулом потоков
- Асинхронное выполнение задач

#### logging
- Структурированное логирование
- Запись в файлы
- Уровни логирования

#### os
- Операции файловой системы
- Управление путями

#### glob
- Поиск файлов по шаблонам
- Рекурсивный обход директорий

#### random
- Случайный выбор User-Agent
- Ротация параметров запросов

---

## 4. Детальные Функциональные Спецификации

### 4.1 Модуль Сетевого Взаимодействия

#### 4.1.1 Создание HTTP-Клиента
**Функция**: create_scraper()

**Входные параметры**:
- `use_cloudscraper` (boolean): Флаг использования Cloudscraper
- `use_proxy` (boolean): Флаг использования прокси

**Логика**:
1. ЕСЛИ use_cloudscraper = TRUE:
   - Создать scraper через cloudscraper.create_scraper()
2. ИНАЧЕ:
   - Импортировать модуль requests
   - Создать Session объект

3. ЕСЛИ use_proxy = TRUE:
   - Установить прокси-конфигурацию для HTTP и HTTPS
   - Формат прокси: "http://your-proxy-server:port"

**Выходные данные**: Объект HTTP-клиента (scraper или session)

#### 4.1.2 Ротация User-Agent

**Список User-Agent**:
- User-Agent 1: Chrome 58 на Windows 10
  - Строка: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
- User-Agent 2: Safari 14 на macOS
  - Строка: "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15"

**Механизм ротации**:
- Случайный выбор из предопределенного списка при каждом запросе
- Установка через заголовок 'User-Agent'

#### 4.1.3 Загрузка XML-Данных
**Функция**: fetch_xml()

**Входные параметры**:
- `url` (string): URL XML-файла
- `use_cloudscraper` (boolean): Использовать Cloudscraper
- `use_proxy` (boolean): Использовать прокси

**Последовательность операций**:
1. Создать HTTP-клиент через create_scraper()
2. Установить случайный User-Agent
3. Выполнить GET-запрос к URL
4. Проверить статус ответа (raise_for_status)
5. Парсить content в ElementTree объект
6. При ошибке: логировать и вернуть None

**Выходные данные**: ElementTree.Element или None

**Обработка ошибок**: Catch-all блок с логированием полного traceback

#### 4.1.4 Загрузка и Декомпрессия GZ-Файлов
**Функция**: decompress_gz()

**Входные параметры**: Идентичны fetch_xml()

**Последовательность операций**:
1. Создать HTTP-клиент
2. Установить случайный User-Agent
3. Выполнить GET-запрос с параметром stream=True
4. Проверить статус ответа
5. Открыть response.raw через gzip.open() в режиме 'rb'
6. Прочитать полное содержимое
7. Парсить в ElementTree объект
8. При ошибке: логировать и вернуть None

**Выходные данные**: ElementTree.Element или None

**Особенности**:
- Потоковая обработка для экономии памяти
- Автоматическая декомпрессия

---

### 4.2 Модуль Парсинга XML

#### 4.2.1 XML Namespace Specification
**Пространство имен**:
- Префикс: 'sm'
- URI: 'http://www.sitemaps.org/schemas/sitemap/0.9'

**Использование во всех XPath-запросах**

#### 4.2.2 Извлечение URL из Sitemap
**Функция**: process_sitemap()

**Входные параметры**:
- `url` (string): URL карты сайта
- `is_compressed` (boolean): Флаг сжатия
- `use_cloudscraper` (boolean)
- `use_proxy` (boolean)

**Логика обработки**:

1. **Загрузка данных**:
   - ЕСЛИ is_compressed = TRUE: вызвать decompress_gz()
   - ИНАЧЕ: вызвать fetch_xml()

2. **Инициализация структур данных**:
   - sitemap_urls = [] (список вложенных карт)
   - page_urls = [] (список страниц)
   - namespace = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}

3. **Извлечение вложенных карт**:
   - XPath: './/sm:sitemap' с namespace
   - Для каждого элемента:
     - Найти дочерний элемент 'sm:loc'
     - Извлечь текстовое содержимое
     - Добавить в sitemap_urls

4. **Извлечение URL страниц**:
   - XPath: './/sm:url' с namespace
   - Для каждого элемента:
     - Найти дочерний элемент 'sm:loc'
     - Извлечь текстовое содержимое
     - Добавить в page_urls

5. **Сохранение результатов**:
   - ЕСЛИ page_urls не пуст:
     - Вызвать save_urls(url, page_urls)

**Выходные данные**: Кортеж (sitemap_urls, page_urls)

**Обработка ошибок**: При ошибке загрузки вернуть ([], [])

---

### 4.3 Модуль Управления Обработкой

#### 4.3.1 Обработка Множественных Карт Сайтов
**Функция**: process_all_sitemaps()

**Входные параметры**:
- `start_urls` (list): Начальные URL для обработки
- `use_cloudscraper` (boolean)
- `use_proxy` (boolean)

**Структуры данных**:
- `all_sitemap_urls` (set): Множество всех обнаруженных карт
- `all_page_urls` (set): Множество всех обнаруженных страниц
- `queue` (list): Очередь URL для обработки

**Алгоритм обработки**:

1. **Инициализация**:
   - Создать пустые множества для sitemap и page URLs
   - Скопировать start_urls в очередь
   - Создать ThreadPoolExecutor с max_workers=5

2. **Основной цикл обработки**:
   ```
   ПОКА queue не пуста:
     1. Извлечь первый URL из очереди (FIFO)
     2. Определить is_compressed (проверка окончания на '.xml.gz')
     3. Создать Future через executor.submit():
        - Функция: process_sitemap
        - Параметры: current_url, is_compressed, use_cloudscraper, use_proxy
     4. Дождаться результата Future
     5. Обработать результат:
        - Добавить sitemap_urls в all_sitemap_urls
        - Добавить page_urls в all_page_urls
        - Добавить в queue только новые sitemap_urls (не в all_sitemap_urls)
     6. При ошибке: логировать и продолжить
   ```

3. **Финализация**:
   - ЕСЛИ all_sitemap_urls не пусто:
     - Сохранить индекс всех карт в файл "sitemap_index.txt"
   - Вернуть (all_sitemap_urls, all_page_urls)

**Особенности реализации**:
- Использование множеств предотвращает дублирование
- Динамическое расширение очереди при обнаружении вложенных карт
- Параллельная обработка через пул потоков
- Каждая задача обрабатывается асинхронно

**Выходные данные**: Кортеж (set всех sitemap URLs, set всех page URLs)

---

### 4.4 Модуль Файловых Операций

#### 4.4.1 Сохранение URL в Файлы
**Функция**: save_urls()

**Входные параметры**:
- `url` (string): Исходный URL карты сайта
- `urls` (list): Список извлеченных URL

**Алгоритм генерации имени файла**:
1. Разбить URL по разделителю '/'
2. Взять последний элемент (имя файла)
3. Разбить по '.'
4. Взять первый элемент (имя без расширения)
5. Добавить расширение '.txt'

**Формат выходного файла**:
```
Source URL: <исходный_url>
<url_1>
<url_2>
...
<url_n>
```

**Последовательность операций**:
1. Сгенерировать имя файла
2. Открыть файл в режиме записи ('w')
3. Записать строку с исходным URL
4. Итерировать по списку urls:
   - Записать каждый URL на новой строке
5. Логировать количество сохраненных URL
6. При ошибке: логировать exception

**Обработка ошибок**: Try-except с логированием

#### 4.4.2 Чтение URL из Файла
**Функция**: read_urls_from_file()

**Входные параметры**:
- `file_path` (string): Путь к файлу со списком URL

**Логика обработки**:
1. Открыть файл в режиме чтения ('r')
2. Прочитать все строки
3. Для каждой строки:
   - Удалить пробелы в начале и конце (strip)
   - Включить только непустые строки
4. Вернуть список URL
5. При ошибке: логировать и вернуть пустой список

**Выходные данные**: List[string]

#### 4.4.3 Поиск XML-Файлов в Директории
**Функция**: find_xml_files_in_directory()

**Входные параметры**:
- `directory` (string): Путь к директории

**Поисковые паттерны**:
1. `*.xml` - все XML файлы
2. `*.xml.gz` - все сжатые XML файлы

**Логика**:
1. Выполнить glob.glob для паттерна '*.xml' в указанной директории
2. Выполнить glob.glob для паттерна '*.xml.gz' в указанной директории
3. Объединить результаты обоих поисков
4. Вернуть объединенный список
5. При ошибке: логировать и вернуть пустой список

**Выходные данные**: List[string] - список полных путей к файлам

---

### 4.5 Модуль Логирования

#### 4.5.1 Конфигурация Логирования

**Параметры**:
- **Файл логов**: 'sitemap_processing.log'
- **Уровень логирования**: DEBUG
- **Формат сообщений**: '%(asctime)s - %(levelname)s - %(message)s'

**Компоненты формата**:
- `%(asctime)s`: Временная метка в формате YYYY-MM-DD HH:MM:SS
- `%(levelname)s`: Уровень логирования (DEBUG, INFO, ERROR, etc.)
- `%(message)s`: Текст сообщения

**Типы логируемых событий**:

1. **INFO уровень**:
   - Начало обработки с количеством карт сайтов
   - Успешное сохранение URL с указанием количества и имени файла
   - Завершение обработки с итоговой статистикой

2. **ERROR уровень**:
   - Ошибки загрузки URL с полным URL и описанием ошибки
   - Ошибки декомпрессии с URL и описанием
   - Ошибки сохранения файлов
   - Ошибки чтения файлов с путем
   - Ошибки сканирования директорий
   - Ошибки обработки конкретных URL в пуле потоков
   - Отсутствие валидных источников данных

3. **DEBUG уровень**:
   - Все события уровня INFO и ERROR
   - Детали внутренней работы системы

**Ротация логов**: Не реализована (файл растет неограниченно)

---

### 4.6 Интерфейс Командной Строки

#### 4.6.1 Аргументы CLI

**Позиционные аргументы**: Отсутствуют

**Опциональные аргументы**:

1. **--url**
   - Тип: string
   - Назначение: Прямой URL главной карты сайта
   - Пример: `--url https://example.com/sitemap_index.xml`

2. **--file**
   - Тип: string
   - Назначение: Путь к файлу со списком URL
   - Формат файла: Один URL на строку
   - Пример: `--file sitemaps.txt`

3. **--directory**
   - Тип: string
   - Назначение: Путь к директории с XML-файлами
   - Обрабатываемые форматы: .xml, .xml.gz
   - Пример: `--directory ./sitemaps/`

4. **--no-cloudscraper**
   - Тип: boolean flag (action='store_true')
   - Назначение: Отключить Cloudscraper
   - По умолчанию: Cloudscraper включен
   - Эффект: Использовать стандартный requests.Session

5. **--proxy**
   - Тип: boolean flag (action='store_true')
   - Назначение: Включить поддержку прокси
   - По умолчанию: Прокси отключены
   - Конфигурация прокси: В исходном коде функции create_scraper

**Валидация аргументов**:
- Хотя бы один из --url, --file, --directory должен быть указан
- При отсутствии валидных источников:
  - Логировать ошибку
  - Показать справку (parser.print_help())
  - Завершить с кодом 1

#### 4.6.2 Логика Основной Функции

**Последовательность выполнения**:

1. **Парсинг аргументов**:
   - Создать ArgumentParser с описанием
   - Добавить все аргументы
   - Парсить args

2. **Сбор URL для обработки**:
   - Инициализировать пустой список urls_to_process
   - ЕСЛИ args.url существует: добавить в список
   - ЕСЛИ args.file существует:
     - Прочитать через read_urls_from_file()
     - Добавить результат в список
   - ЕСЛИ args.directory существует:
     - Найти файлы через find_xml_files_in_directory()
     - Добавить результат в список

3. **Валидация**:
   - ЕСЛИ urls_to_process пуст:
     - Логировать ошибку
     - Показать справку
     - Завершить программу

4. **Обработка**:
   - Логировать начало с количеством карт
   - Вызвать process_all_sitemaps():
     - Параметр 1: urls_to_process
     - Параметр 2: NOT args.no_cloudscraper
     - Параметр 3: args.proxy
   - Получить результаты: (sitemaps, pages)

5. **Финализация**:
   - Логировать завершение
   - Логировать количество найденных sitemap URLs
   - Логировать количество найденных page URLs

---

## 5. Структура Данных

### 5.1 Внутренние Структуры

#### 5.1.1 Очередь Обработки
- **Тип**: list
- **Операции**: FIFO (First In, First Out)
- **Содержимое**: Строки URL
- **Динамика**: Расширяется при обнаружении вложенных карт

#### 5.1.2 Множества URL
- **all_sitemap_urls**: set[string]
  - Назначение: Отслеживание всех обнаруженных карт сайтов
  - Использование: Предотвращение дублирования и повторной обработки

- **all_page_urls**: set[string]
  - Назначение: Коллекция всех извлеченных URL страниц
  - Использование: Уникальность URL в итоговом результате

### 5.2 Формат Выходных Файлов

#### 5.2.1 Файлы с Извлеченными URL
**Расширение**: .txt
**Кодировка**: UTF-8 (по умолчанию Python)
**Структура**:
- Строка 1: "Source URL: <исходный_url>"
- Строки 2-N: По одному URL на строку

**Пример**:
```
Source URL: https://example.com/sitemap-posts.xml
https://example.com/post-1
https://example.com/post-2
https://example.com/post-3
```

#### 5.2.2 Файл Индекса Карт
**Имя файла**: sitemap_index.txt
**Назначение**: Список всех обнаруженных вложенных карт
**Условие создания**: Если all_sitemap_urls не пусто
**Формат**: Идентичен формату файлов с URL

#### 5.2.3 Файл Логов
**Имя файла**: sitemap_processing.log
**Формат**: Текстовый, построчный
**Структура строки**: "YYYY-MM-DD HH:MM:SS - LEVEL - Message"

---

## 6. Спецификации Протоколов и Форматов

### 6.1 HTTP/HTTPS Протокол

#### 6.1.1 HTTP-Запросы
- **Метод**: GET
- **Протоколы**: HTTP/1.1, HTTPS
- **Заголовки**:
  - User-Agent: Ротируемый из предопределенного списка
  - Дополнительные заголовки от Cloudscraper (если включен)

#### 6.1.2 Обработка Ответов
- **Проверка статуса**: response.raise_for_status()
  - Успешные коды: 200-299
  - Ошибочные коды: 400-599 вызывают HTTPError
- **Обработка контента**: response.content (bytes)
- **Потоковая загрузка**: stream=True для .gz файлов

### 6.2 XML Sitemap Protocol

#### 6.2.1 Стандарт
- **Спецификация**: sitemaps.org protocol
- **Namespace URI**: http://www.sitemaps.org/schemas/sitemap/0.9
- **Префикс**: sm

#### 6.2.2 Поддерживаемые Элементы

**Элемент Sitemap Index**:
```
<sitemap>
  <loc>URL</loc>
  [другие элементы игнорируются]
</sitemap>
```

**Элемент URL**:
```
<url>
  <loc>URL</loc>
  [другие элементы игнорируются]
</url>
```

**Игнорируемые элементы**:
- `<lastmod>` - дата модификации
- `<changefreq>` - частота изменений
- `<priority>` - приоритет
- Любые другие дочерние элементы

#### 6.2.3 Корневые Элементы
- `<sitemapindex>` - для индексных файлов
- `<urlset>` - для файлов с URL

**Обработка**: Система ищет элементы независимо от корневого элемента (использует './/')

### 6.3 Формат GZIP

#### 6.3.1 Спецификация
- **Расширение**: .xml.gz
- **Алгоритм**: GZIP (RFC 1952)
- **Режим чтения**: Binary ('rb')

#### 6.3.2 Обработка
- Автоматическое определение по расширению файла
- Потоковая декомпрессия
- Парсинг декомпрессированного XML

---

## 7. Спецификации Производительности

### 7.1 Многопоточность

#### 7.1.1 Конфигурация ThreadPoolExecutor
- **Количество рабочих потоков**: 5
- **Тип пула**: ThreadPoolExecutor (threading-based)
- **Управление задачами**: Future-based

#### 7.1.2 Параллелизм
- **Модель**: Ограниченный параллелизм (до 5 одновременных задач)
- **Синхронизация**: Неявная через Future.result()
- **Безопасность потоков**: Использование неизменяемых структур и thread-safe операций

### 7.2 Оптимизации

#### 7.2.1 Предотвращение Дублирования
- **Механизм**: Использование множеств (set)
- **Проверка**: Перед добавлением в очередь
- **Эффект**: Каждая карта обрабатывается только один раз

#### 7.2.2 Управление Памятью
- **Потоковая загрузка**: Для .gz файлов
- **Очистка**: Автоматическая сборка мусора Python
- **Накопление**: Все URL хранятся в памяти до завершения

---

## 8. Обработка Ошибок

### 8.1 Стратегия Обработки Ошибок

#### 8.1.1 Уровень Сетевых Запросов
- **Обработчик**: try-except блок
- **Действие при ошибке**:
  1. Логировать полное сообщение об ошибке
  2. Вернуть None
  3. Продолжить обработку других URL

**Типы обрабатываемых ошибок**:
- HTTPError - неверные HTTP-коды
- ConnectionError - проблемы подключения
- Timeout - превышение времени ожидания
- RequestException - общие ошибки запросов

#### 8.1.2 Уровень Парсинга XML
- **Обработчик**: Косвенно через try-except в fetch_xml/decompress_gz
- **Действие**: Логирование и возврат None

**Типы ошибок**:
- ParseError - некорректный XML
- ValueError - неверная структура данных

#### 8.1.3 Уровень Файловых Операций
- **Обработчик**: try-except блок
- **Действие при ошибке**:
  1. Логировать ошибку
  2. Для чтения: вернуть пустой список
  3. Для записи: пропустить операцию

**Типы ошибок**:
- FileNotFoundError - файл не найден
- PermissionError - нет прав доступа
- IOError - ошибки ввода-вывода

#### 8.1.4 Уровень Пула Потоков
- **Обработчик**: try-except вокруг future.result()
- **Действие**: Логировать и продолжить обработку очереди

### 8.2 Модель Отказоустойчивости
- **Принцип**: Fail-soft (мягкий отказ)
- **Поведение**: При ошибке обработки одной карты продолжить обработку остальных
- **Финализация**: Всегда выполняется независимо от ошибок

---

## 9. Конфигурация и Настройка

### 9.1 Хардкодированные Параметры

#### 9.1.1 Многопоточность
- **Параметр**: max_workers
- **Значение**: 5
- **Расположение**: Функция process_all_sitemaps

#### 9.1.2 Прокси
- **Параметр**: proxy URL
- **Значение по умолчанию**: "http://your-proxy-server:port"
- **Расположение**: Функция create_scraper
- **Требование**: Замена на реальный прокси-сервер

#### 9.1.3 User-Agent
- **Параметр**: USER_AGENTS список
- **Количество**: 2 элемента
- **Расположение**: Глобальная константа
- **Расширяемость**: Возможно добавление дополнительных UA

### 9.2 Параметры CLI
Все параметры настраиваются через аргументы командной строки (см. раздел 4.6)

---

## 10. Спецификации Развертывания

### 10.1 Структура Пакета

#### 10.1.1 Файловая Иерархия
```
sitemap-extract/
├── sitemap_extract/
│   ├── __init__.py
│   └── sitemap_extract.py
├── setup.py
├── requirements.txt
├── README.md
└── LICENSE
```

#### 10.1.2 Назначение Файлов

**setup.py**:
- Конфигурация setuptools
- Определение пакета
- Зависимости
- Entry points для CLI

**requirements.txt**:
- Список внешних зависимостей
- Фиксация версий

**__init__.py**:
- Маркер пакета Python
- Инициализация модуля
- Содержимое: пустой файл

**sitemap_extract.py**:
- Основной исполняемый модуль
- Вся бизнес-логика

### 10.2 Установка

#### 10.2.1 Через pip и requirements.txt
**Команда**: `pip install -r requirements.txt`

**Установленные пакеты**:
- cloudscraper==1.2.58
- argparse==1.4.0

#### 10.2.2 Через setup.py
**Команда**: `pip install .` или `python setup.py install`

**Эффект**:
- Установка пакета sitemap_extract
- Регистрация console script entry point
- Доступность команды: `sitemap_extract`

### 10.3 Запуск

#### 10.3.1 Как Модуль
**Команда**: `python -m sitemap_extract.sitemap_extract [arguments]`

**Требования**:
- Python 3.x в PATH
- Установленные зависимости
- Корректная структура пакета

#### 10.3.2 Как Установленная Команда
**Команда**: `sitemap_extract [arguments]`

**Требования**:
- Установка через setup.py
- Entry point корректно зарегистрирован

---

## 11. Спецификации Безопасности

### 11.1 Антибот-Защита

#### 11.1.1 Cloudscraper
- **Назначение**: Обход Cloudflare и подобных систем
- **Механизм**:
  - Выполнение JavaScript challenge
  - Обработка cookies
  - Эмуляция браузера

#### 11.1.2 User-Agent Ротация
- **Назначение**: Имитация различных браузеров
- **Механизм**: Случайный выбор при каждом запросе
- **Эффект**: Снижение вероятности блокировки

### 11.2 Конфиденциальность

#### 11.2.1 Прокси-Поддержка
- **Назначение**: Анонимизация запросов
- **Конфигурация**: Через параметр use_proxy
- **Протоколы**: HTTP и HTTPS

### 11.3 Уязвимости и Ограничения

#### 11.3.1 Хардкодированные Прокси
- **Проблема**: Прокси-URL в исходном коде
- **Риск**: Раскрытие в репозитории
- **Рекомендация**: Вынести в переменные окружения

#### 11.3.2 Обработка Недоверенного XML
- **Проблема**: Парсинг XML из внешних источников
- **Риск**: XML External Entity (XXE) атаки
- **Смягчение**: ElementTree по умолчанию защищен от XXE

#### 11.3.3 Неограниченный Рост Памяти
- **Проблема**: Все URL хранятся в памяти
- **Риск**: Исчерпание памяти на больших датасетах
- **Ограничение**: Зависит от доступной RAM

---

## 12. Тестирование и Валидация

### 12.1 Сценарии Использования

#### 12.1.1 Обработка Одиночной Карты
**Входные данные**: Один URL через --url
**Ожидаемый результат**:
- Загрузка и парсинг XML
- Извлечение всех URL
- Сохранение в .txt файл
- Рекурсивная обработка вложенных карт
- Логирование всех операций

#### 12.1.2 Пакетная Обработка из Файла
**Входные данные**: Файл со списком URL через --file
**Ожидаемый результат**:
- Чтение всех URL из файла
- Обработка каждого URL
- Создание отдельного .txt для каждой карты
- Общий индексный файл

#### 12.1.3 Сканирование Директории
**Входные данные**: Путь к директории через --directory
**Ожидаемый результат**:
- Поиск всех .xml и .xml.gz файлов
- Обработка локальных файлов (требует адаптации для file://)
- Извлечение URL

#### 12.1.4 Обработка Сжатых Файлов
**Входные данные**: URL на .xml.gz файл
**Ожидаемый результат**:
- Автоматическое определение сжатия
- Декомпрессия
- Корректный парсинг
- Идентичный результат несжатому XML

#### 12.1.5 Глубокая Вложенность
**Входные данные**: Карта с множественными уровнями вложенности
**Ожидаемый результат**:
- Обработка всех уровней
- Корректное извлечение всех URL
- Отсутствие дублирования
- Завершение после полного обхода

### 12.2 Тестовые Условия

#### 12.2.1 Сетевые Ошибки
**Сценарий**: Недоступный URL
**Ожидаемое поведение**:
- Логирование ошибки
- Возврат None из fetch_xml
- Продолжение обработки других URL

#### 12.2.2 Некорректный XML
**Сценарий**: Невалидный XML-контент
**Ожидаемое поведение**:
- Логирование ошибки парсинга
- Пропуск файла
- Продолжение обработки

#### 12.2.3 Отсутствие Прав на Запись
**Сценарий**: Нет прав на создание файлов
**Ожидаемое поведение**:
- Логирование ошибки IOError
- Пропуск сохранения
- Продолжение обработки

---

## 13. Ограничения и Известные Проблемы

### 13.1 Функциональные Ограничения

#### 13.1.1 Обработка Директорий
- **Проблема**: find_xml_files_in_directory возвращает пути к локальным файлам
- **Ограничение**: fetch_xml ожидает HTTP URL
- **Последствие**: Обработка локальных файлов из директории не функционирует корректно

#### 13.1.2 Конфигурация Прокси
- **Проблема**: Прокси URL хардкоден в коде
- **Ограничение**: Невозможность изменения без модификации кода
- **Рекомендация**: Реализовать через CLI аргумент или конфиг-файл

#### 13.1.3 Ограниченная Ротация User-Agent
- **Проблема**: Только 2 User-Agent в списке
- **Ограничение**: Низкое разнообразие для больших объемов
- **Рекомендация**: Расширить список или загружать из файла

### 13.2 Производительность

#### 13.2.1 Фиксированный Пул Потоков
- **Ограничение**: max_workers=5 хардкоден
- **Проблема**: Невозможность адаптации под ресурсы системы
- **Рекомендация**: Сделать настраиваемым через CLI

#### 13.2.2 Последовательная Обработка Очереди
- **Ограничение**: queue.pop(0) с немедленным submit
- **Эффект**: Неоптимальное использование пула при быстрых задачах
- **Рекомендация**: Batch-обработка задач

#### 13.2.3 Накопление в Памяти
- **Проблема**: Все URL хранятся в памяти до завершения
- **Ограничение**: Проблемы на датасетах с миллионами URL
- **Рекомендация**: Потоковая запись в файл

### 13.3 Надежность

#### 13.3.1 Отсутствие Повторных Попыток
- **Проблема**: Временные сетевые ошибки не обрабатываются
- **Ограничение**: Потеря данных при флуктуациях сети
- **Рекомендация**: Реализовать retry-логику с экспоненциальной задержкой

#### 13.3.2 Нет Сохранения Состояния
- **Проблема**: При сбое теряется весь прогресс
- **Ограничение**: Невозможность возобновления обработки
- **Рекомендация**: Периодическое сохранение прогресса

---

## 14. Расширяемость и Модификации

### 14.1 Точки Расширения

#### 14.1.1 Добавление Новых Источников
**Расположение**: Основная функция __main__
**Механизм**: Добавление нового CLI аргумента и функции чтения
**Пример**: Чтение из базы данных, API endpoint

#### 14.1.2 Дополнительные Форматы
**Расположение**: Функции fetch_xml, decompress_gz
**Механизм**: Добавление новых функций декодирования
**Примеры**: .zip архивы, .bz2 сжатие

#### 14.1.3 Извлечение Метаданных
**Расположение**: Функция process_sitemap
**Механизм**: Расширение парсинга для дополнительных элементов
**Примеры**: lastmod, changefreq, priority

#### 14.1.4 Форматы Вывода
**Расположение**: Функция save_urls
**Механизм**: Создание альтернативных функций сохранения
**Примеры**: JSON, CSV, SQLite

### 14.2 Рекомендуемые Улучшения

#### 14.2.1 Конфигурационный Файл
**Назначение**: Централизованное управление настройками
**Формат**: YAML, JSON или INI
**Параметры**:
- Прокси-серверы
- User-Agent список
- Количество потоков
- Таймауты
- Retry-параметры

#### 14.2.2 Прогресс-Индикатор
**Назначение**: Визуализация прогресса обработки
**Реализация**: tqdm или rich библиотеки
**Отображение**:
- Процент выполнения
- Текущая обрабатываемая карта
- Скорость обработки
- ETA

#### 14.2.3 Статистика и Отчеты
**Назначение**: Детальная аналитика обработки
**Данные**:
- Время обработки каждой карты
- Количество ошибок по типам
- Распределение вложенности
- Размеры карт

#### 14.2.4 Валидация URL
**Назначение**: Проверка корректности извлеченных URL
**Проверки**:
- Валидность схемы (http/https)
- Корректность структуры
- Фильтрация дубликатов
- Опциональная нормализация

---

## 15. Зависимости и Совместимость

### 15.1 Python Версии
- **Минимальная**: Python 3.x (точная версия не специфицирована)
- **Рекомендуемая**: Python 3.7+
- **Причина**: Использование f-strings, type hints совместимость

### 15.2 Внешние Зависимости

#### 15.2.1 cloudscraper 1.2.58
**Зависимости cloudscraper**:
- requests
- requests-toolbelt
- pyparsing

**Возможные конфликты**: Версии requests

#### 15.2.2 argparse 1.4.0
**Примечание**: Входит в стандартную библиотеку Python 3.2+
**Необходимость явной установки**: Только для Python < 3.2

### 15.3 Системные Требования

#### 15.3.1 Операционная Система
- **Поддержка**: Linux, Windows, macOS
- **Зависимости**: Нет платформо-специфичных вызовов

#### 15.3.2 Ресурсы
- **RAM**: Зависит от размера датасета (рекомендуется 2GB+)
- **Диск**: Достаточно для хранения выходных файлов
- **Сеть**: Стабильное подключение для загрузки карт

---

## 16. Потоки Данных

### 16.1 Основной Поток Обработки

```
[Источник входных данных]
        ↓
[Аргументы CLI: --url / --file / --directory]
        ↓
[Сбор списка стартовых URL]
        ↓
[Инициализация очереди обработки]
        ↓
[ThreadPoolExecutor с 5 потоками]
        ↓
[Цикл обработки очереди] ←────┐
        ↓                      │
[Извлечение URL из очереди]    │
        ↓                      │
[Определение типа: .gz или .xml]│
        ↓                      │
[Создание Future задачи]       │
        ↓                      │
[Параллельная загрузка XML]    │
        ↓                      │
[Парсинг с учетом namespace]   │
        ↓                      │
[Извлечение sitemap и page URLs]│
        ↓                      │
[Добавление новых sitemap в очередь]──┘
        ↓
[Сохранение page URLs в файлы]
        ↓
[Логирование результатов]
        ↓
[Завершение при пустой очереди]
```

### 16.2 Поток Обработки Одной Карты

```
[URL карты сайта]
        ↓
[Проверка расширения: .xml.gz?]
        ↓
    ┌───┴───┐
   ДА      НЕТ
    ↓       ↓
[decompress_gz] [fetch_xml]
    ↓       ↓
    └───┬───┘
        ↓
[Создание HTTP-клиента]
        ↓
[Установка User-Agent]
        ↓
[Конфигурация прокси (если включено)]
        ↓
[GET запрос к URL]
        ↓
[Проверка HTTP статуса]
        ↓
    ┌───┴───┐
 Ошибка   Успех
    ↓       ↓
[Лог + None] [Получение content]
              ↓
          [Декомпрессия (если .gz)]
              ↓
          [Парсинг в ElementTree]
              ↓
          [XPath поиск элементов]
              ↓
          ┌───┴───┐
          ↓       ↓
    [<sitemap>] [<url>]
          ↓       ↓
    [Извлечение] [Извлечение]
    [loc элементов] [loc элементов]
          ↓       ↓
    [sitemap_urls] [page_urls]
          ↓       ↓
          └───┬───┘
              ↓
        [Возврат кортежа]
```

### 16.3 Поток Логирования

```
[Событие в приложении]
        ↓
[Определение уровня: INFO/ERROR/DEBUG]
        ↓
[Форматирование: timestamp + level + message]
        ↓
[Запись в sitemap_processing.log]
        ↓
[Автоматический flush]
```

---

## 17. Детальные Алгоритмы

### 17.1 Алгоритм Предотвращения Циклов

**Проблема**: Взаимные ссылки между картами могут создать циклы

**Решение**:
1. Использование множества all_sitemap_urls
2. Проверка перед добавлением в очередь:
   ```
   ДЛЯ каждого url В sitemap_urls:
     ЕСЛИ url НЕ В all_sitemap_urls:
       Добавить url в all_sitemap_urls
       Добавить url в queue
     ИНАЧЕ:
       Пропустить (уже обработан или в очереди)
   ```

**Гарантия**: Каждый URL обрабатывается максимум один раз

### 17.2 Алгоритм Обработки Вложенности

**Стратегия**: Поиск в ширину (Breadth-First Search)

**Этапы**:
1. Начать с корневой карты (master sitemap)
2. Извлечь все ссылки на уровне 1
3. Добавить в очередь все дочерние карты уровня 1
4. Обработать уровень 1, извлечь ссылки уровня 2
5. Продолжить до достижения листовых карт (без вложенных sitemap)

**Преимущества**:
- Обработка по уровням
- Ранее обнаружение широких структур
- Равномерная загрузка потоков

### 17.3 Алгоритм Генерации Имен Файлов

**Входные данные**: URL карты сайта

**Процесс**:
1. Разбить URL по '/'
   - Пример: "https://example.com/sitemap-posts.xml" → ["https:", "", "example.com", "sitemap-posts.xml"]

2. Взять последний элемент
   - Результат: "sitemap-posts.xml"

3. Разбить по '.'
   - Результат: ["sitemap-posts", "xml"]

4. Взять первый элемент
   - Результат: "sitemap-posts"

5. Конкатенировать с ".txt"
   - Итог: "sitemap-posts.txt"

**Коллизии**: Возможны при одинаковых именах файлов с разных URL (перезапись)

---

## 18. Спецификации Интеграции

### 18.1 Использование как Библиотека

**Импорт**:
```python
from sitemap_extract.sitemap_extract import process_all_sitemaps, read_urls_from_file
```

**Программный вызов**:
```python
urls = ["https://example.com/sitemap.xml"]
sitemaps, pages = process_all_sitemaps(urls, use_cloudscraper=True, use_proxy=False)
```

### 18.2 Расширение Функциональности

#### 18.2.1 Кастомные Обработчики
**Точка внедрения**: После process_all_sitemaps

**Пример использования**:
- Фильтрация URL по паттернам
- Валидация существования страниц
- Извлечение дополнительных метаданных

#### 18.2.2 Альтернативные Парсеры
**Точка внедрения**: Замена fetch_xml/decompress_gz

**Возможности**:
- Использование lxml вместо ElementTree
- Обработка невалидного XML
- Поддержка других форматов (HTML sitemaps)

---

## 19. Документация по Эксплуатации

### 19.1 Запуск Обработки

#### 19.1.1 Одиночный URL
**Команда**:
```bash
python -m sitemap_extract.sitemap_extract --url https://example.com/sitemap_index.xml
```

**Результат**:
- Файл sitemap_index.txt (если есть вложенные карты)
- Файлы для каждой дочерней карты
- sitemap_processing.log

#### 19.1.2 Список URL
**Подготовка файла** (например, urls.txt):
```
https://example1.com/sitemap.xml
https://example2.com/sitemap.xml
https://example3.com/sitemap.xml
```

**Команда**:
```bash
python -m sitemap_extract.sitemap_extract --file urls.txt
```

#### 19.1.3 С Прокси и без Cloudscraper
**Команда**:
```bash
python -m sitemap_extract.sitemap_extract --url https://example.com/sitemap.xml --proxy --no-cloudscraper
```

**Эффект**:
- Использование requests.Session вместо Cloudscraper
- Включение прокси из кода

### 19.2 Мониторинг Выполнения

#### 19.2.1 Просмотр Логов в Реальном Времени
**Команда** (Linux/macOS):
```bash
tail -f sitemap_processing.log
```

**Команда** (Windows):
```bash
Get-Content sitemap_processing.log -Wait
```

#### 19.2.2 Анализ Результатов
**Поиск ошибок**:
```bash
grep "ERROR" sitemap_processing.log
```

**Подсчет обработанных файлов**:
```bash
ls -1 *.txt | wc -l
```

### 19.3 Обработка Результатов

#### 19.3.1 Объединение Всех URL
**Команда** (Linux/macOS):
```bash
cat *.txt | grep -v "^Source URL:" > all_urls.txt
```

**Результат**: Единый файл со всеми извлеченными URL

#### 19.3.2 Подсчет Уникальных URL
**Команда**:
```bash
cat *.txt | grep -v "^Source URL:" | sort -u | wc -l
```

---

## 20. Метрики и Мониторинг

### 20.1 Ключевые Метрики

#### 20.1.1 Производительность
- **Среднее время обработки карты**: Зависит от размера и сети
- **Пропускная способность**: URL/секунда
- **Эффективность параллелизма**: Загрузка потоков

#### 20.1.2 Надежность
- **Процент успешных загрузок**: (Успешные / Всего) * 100
- **Частота ошибок**: Количество ERROR записей в логе
- **Полнота данных**: Извлеченные URL vs ожидаемые

#### 20.1.3 Ресурсы
- **Использование памяти**: Мониторинг через системные утилиты
- **Дисковое пространство**: Размер выходных файлов + логи
- **Сетевой трафик**: Объем загруженных данных

### 20.2 Логируемые События

#### 20.2.1 Информационные (INFO)
- "Starting processing of N sitemap(s)"
- "Saved N URLs to filename.txt"
- "Processing complete. Found: X sitemap URLs, Y page URLs"

#### 20.2.2 Ошибочные (ERROR)
- "Failed to fetch URL {url}: {error}"
- "Failed to decompress URL {url}: {error}"
- "Failed to save URLs: {error}"
- "Error reading file {file_path}: {error}"
- "Error scanning directory {directory}: {error}"
- "Error processing {url}: {error}"
- "No valid input sources provided"

---

## Заключение

Данный документ содержит полные технические спецификации приложения Sitemap Extract. Документация охватывает все аспекты системы: от архитектуры и алгоритмов до конкретных параметров конфигурации и форматов данных.

### Ключевые Моменты для Реализации

1. **Многопоточная архитектура** с ThreadPoolExecutor и 5 потоками
2. **Поддержка вложенности** через очередь с динамическим расширением
3. **Обработка форматов**: XML и XML.GZ
4. **Антибот-защита**: Cloudscraper + ротация User-Agent
5. **Множественные источники**: URL, файл, директория
6. **Отказоустойчивость**: Fail-soft модель с логированием
7. **Предотвращение дублирования**: Использование множеств
8. **Детальное логирование**: Все операции записываются в лог

### Технологический Минимум

- Python 3.7+
- cloudscraper 1.2.58
- Стандартная библиотека: xml.etree.ElementTree, gzip, concurrent.futures, logging, argparse, os, glob, random

### Критические Параметры

- **max_workers**: 5 потоков
- **Namespace**: http://www.sitemaps.org/schemas/sitemap/0.9
- **Формат лога**: "%(asctime)s - %(levelname)s - %(message)s"
- **Файл логов**: sitemap_processing.log
- **Расширения файлов**: .txt для вывода

Используя эти спецификации, возможно полное воссоздание функциональности приложения без доступа к исходному коду.
