# Задачи Фазы 2: Основные Модули (Network, Parser, File Operations)

## Обзор Фазы

Эта фаза реализует три критических модуля, которые составляют ядро функциональности приложения:
- **Network**: Сетевое взаимодействие, HTTP запросы, обход антибот-систем
- **Parser**: Парсинг XML sitemap, извлечение URL
- **File Operations**: Работа с файловой системой, чтение/запись

**Зависимости**: Фаза 1 (инфраструктура должна быть готова)

**Результат**: Три полностью функциональных модуля с тестовым покрытием

---

# МОДУЛЬ NETWORK

## Задача 2.1: Создание Базового HTTP Клиента

### Цель
Реализовать функцию create_scraper для создания настроенного HTTP клиента с поддержкой Cloudscraper и прокси.

### Детали Реализации

1. **Структура функции**
   - Имя: create_scraper
   - Параметры: use_cloudscraper (bool, default=True), use_proxy (bool, default=False)
   - Возврат: Union[CloudScraper, requests.Session]

2. **Логика выбора библиотеки**
   - ЕСЛИ use_cloudscraper == True: использовать cloudscraper.create_scraper()
   - ИНАЧЕ: импортировать requests, создать requests.Session()
   - Условный импорт requests только при необходимости

3. **Конфигурация прокси**
   - ЕСЛИ use_proxy == True: установить прокси
   - Формат: scraper.proxies.update({'http': proxy_url, 'https': proxy_url})
   - Использовать DEFAULT_PROXY из constants (placeholder)
   - Добавить WARNING комментарий о необходимости конфигурации

4. **Обработка ошибок**
   - Импорт requests может упасть если не установлен
   - Обернуть в try-except с информативным сообщением
   - Логировать создание scraper

### Сценарии Тестирования

1. **Тест с Cloudscraper**
   - Вызов create_scraper(use_cloudscraper=True)
   - Возврат должен быть экземпляром CloudScraper
   - Объект должен иметь метод get()

2. **Тест с requests.Session**
   - Вызов create_scraper(use_cloudscraper=False)
   - Возврат должен быть экземпляром requests.Session
   - Объект должен иметь метод get()

3. **Тест с прокси**
   - Вызов create_scraper(use_proxy=True)
   - scraper.proxies должен содержать 'http' и 'https' ключи
   - Значения должны быть URL прокси

4. **Тест без прокси**
   - Вызов create_scraper(use_proxy=False)
   - scraper.proxies должен быть пустым или не иметь кастомных прокси

5. **Тест недоступности requests**
   - Mock отсутствие requests модуля
   - Должно подняться понятное исключение

### Потенциальные Проблемы

1. **Хардкоженный прокси**
   - DEFAULT_PROXY = "http://your-proxy-server:port" не функционален
   - Пользователь должен изменить код для работающего прокси
   - Рекомендация: вынести в переменные окружения или config файл

2. **Версии библиотек**
   - cloudscraper 1.2.58 старая версия (2020)
   - Может не работать с современными антибот-системами
   - API может отличаться от новых версий

3. **Условный импорт**
   - Импорт requests внутри функции неидиоматичен
   - Может быть неочевидно для других разработчиков
   - Проблемы с IDE автодополнением

4. **Тестирование**
   - Тестирование реальных прокси затруднительно
   - Mock Cloudscraper может быть сложным
   - Интеграционные тесты требуют сети

### Критерии Принятия

- [ ] Функция create_scraper реализована
- [ ] Поддержка Cloudscraper работает
- [ ] Поддержка requests.Session работает
- [ ] Прокси конфигурируется правильно
- [ ] Функция документирована
- [ ] Unit тесты написаны и проходят

---

## Задача 2.2: Реализация Ротации User-Agent

### Цель
Создать функцию для случайного выбора User-Agent из списка для снижения вероятности блокировки.

### Детали Реализации

1. **Структура функции**
   - Имя: get_random_user_agent
   - Параметры: нет
   - Возврат: str (User-Agent строка)

2. **Логика выбора**
   - Импортировать random.choice
   - Импортировать USER_AGENTS из constants
   - Вернуть random.choice(USER_AGENTS)

3. **Валидация**
   - Убедиться что USER_AGENTS не пустой
   - Опционально: проверка формата User-Agent строки
   - Логировать выбранный UA (на уровне DEBUG)

4. **Расширяемость**
   - Структура позволяет легко добавить больше UA
   - Возможность загрузки UA из файла в будущем
   - Документировать как расширить список

### Сценарии Тестирования

1. **Тест возврата строки**
   - Вызов get_random_user_agent()
   - Возврат должен быть строкой
   - Строка не должна быть пустой

2. **Тест членства**
   - Результат должен быть в USER_AGENTS
   - Никаких других значений не возвращается

3. **Тест случайности**
   - Многократный вызов (100+ раз)
   - Должны быть получены разные UA
   - Распределение примерно равномерное (для большого списка)

4. **Тест с одним UA**
   - Mock USER_AGENTS с одним элементом
   - Всегда должен вернуться этот элемент
   - Нет ошибок

5. **Тест с пустым списком**
   - Mock USER_AGENTS = []
   - Должна подняться ошибка
   - Или вернуть дефолтный UA

### Потенциальные Проблемы

1. **Ограниченное разнообразие**
   - Только 2 User-Agent в списке по умолчанию
   - Легко обнаруживаемы как паттерн
   - Рекомендация: расширить до 10-20 современных UA

2. **Устаревшие User-Agents**
   - Chrome 58 (2017) и Safari 14 (2020) устарели
   - Современные сайты могут их блокировать
   - Нужно периодически обновлять

3. **Отсутствие разнообразия платформ**
   - Только Windows и macOS
   - Нет Linux, Android, iOS
   - Нет различных браузеров (Firefox, Edge, Opera)

4. **Предсказуемость**
   - random.choice использует системный RNG
   - Может быть предсказуемо при seed
   - Для критичных случаев использовать secrets

### Критерии Принятия

- [ ] Функция get_random_user_agent реализована
- [ ] Возвращает случайный UA из списка
- [ ] Валидация входных данных
- [ ] Функция документирована
- [ ] Тесты написаны и проходят

---

## Задача 2.3: Реализация Функции Загрузки XML

### Цель
Создать функцию fetch_xml для загрузки XML файла по URL и парсинга в ElementTree.

### Детали Реализации

1. **Сигнатура функции**
   - Имя: fetch_xml
   - Параметры: url (str), use_cloudscraper (bool), use_proxy (bool)
   - Возврат: Optional[ET.Element] (None при ошибке)

2. **Последовательность операций**
   - Создать scraper через create_scraper(use_cloudscraper, use_proxy)
   - Получить User-Agent через get_random_user_agent()
   - Установить header: scraper.headers['User-Agent'] = user_agent
   - Выполнить GET запрос: response = scraper.get(url)
   - Проверить статус: response.raise_for_status()
   - Парсить: ET.fromstring(response.content)
   - Вернуть корневой элемент

3. **Обработка ошибок**
   - Обернуть все в try-except блок
   - Ловить HTTPError, ConnectionError, Timeout, RequestException
   - Ловить ET.ParseError
   - Логировать ошибку с полным URL и traceback
   - Вернуть None при любой ошибке

4. **Логирование**
   - DEBUG: начало загрузки URL
   - DEBUG: используемый User-Agent
   - INFO: успешная загрузка (размер content)
   - ERROR: детали ошибки

### Сценарии Тестирования

1. **Тест с валидным XML**
   - Mock requests.get с валидным XML ответом
   - Должен вернуться ET.Element
   - Element должен иметь корректную структуру

2. **Тест с невалидным XML**
   - Mock response с невалидным XML
   - Должен вернуться None
   - Должно быть залогировано ParseError

3. **Тест HTTP ошибок**
   - Mock response с status 404
   - Mock response с status 500
   - Должен вернуться None
   - Должны быть залогированы ошибки

4. **Тест сетевых ошибок**
   - Mock ConnectionError
   - Mock Timeout
   - Должен вернуться None
   - Graceful degradation

5. **Тест установки User-Agent**
   - Mock scraper
   - Проверить что headers['User-Agent'] установлен
   - Проверить что значение из get_random_user_agent()

### Потенциальные Проблемы

1. **Большие файлы**
   - response.content загружает весь файл в память
   - Для очень больших sitemap может быть проблема
   - Рекомендация: мониторить использование памяти

2. **Таймауты**
   - Нет явного timeout в запросе
   - Может зависнуть на медленных серверах
   - Рекомендация: добавить timeout=30

3. **Кодировка**
   - response.content это bytes
   - ET.fromstring обрабатывает кодировку автоматически
   - Могут быть проблемы с exotic кодировками

4. **SSL проблемы**
   - Некоторые сайты имеют невалидные сертификаты
   - Cloudscraper может не доверять им
   - Опция verify=False опасна но иногда необходима

5. **Rate limiting**
   - Нет защиты от rate limiting
   - Быстрые последовательные запросы могут блокироваться
   - Рекомендация: добавить задержки

### Критерии Принятия

- [ ] Функция fetch_xml реализована
- [ ] Создается scraper с правильными параметрами
- [ ] User-Agent устанавливается
- [ ] HTTP запрос выполняется
- [ ] XML парсится в ElementTree
- [ ] Ошибки обрабатываются gracefully
- [ ] Логирование на всех этапах
- [ ] Тесты написаны и проходят

---

## Задача 2.4: Реализация Функции Декомпрессии GZ

### Цель
Создать функцию decompress_gz для загрузки и декомпрессии .xml.gz файлов.

### Детали Реализации

1. **Сигнатура функции**
   - Имя: decompress_gz
   - Параметры: идентичны fetch_xml
   - Возврат: Optional[ET.Element]

2. **Последовательность операций**
   - Создать scraper через create_scraper()
   - Получить и установить User-Agent
   - Выполнить GET запрос с stream=True
   - Проверить статус
   - Открыть response.raw с gzip.open(mode='rb')
   - Прочитать полное содержимое
   - Парсить в ElementTree
   - Вернуть корневой элемент

3. **Потоковая обработка**
   - stream=True в requests.get
   - response.raw это file-like объект
   - gzip.open может читать из него напрямую
   - Экономит память для больших файлов

4. **Обработка ошибок**
   - Те же HTTP ошибки что и fetch_xml
   - Дополнительно: gzip.BadGzipFile
   - Логирование специфично для .gz
   - Возврат None при ошибках

### Сценарии Тестирования

1. **Тест с валидным .gz XML**
   - Mock response.raw с gzip сжатым XML
   - Должен вернуться ET.Element
   - Содержимое должно совпадать с оригиналом

2. **Тест с невалидным gzip**
   - Mock response.raw с не-gzip данными
   - Должен вернуться None
   - Должно быть залогировано BadGzipFile

3. **Тест с невалидным XML внутри gz**
   - Mock валидный gzip с невалидным XML
   - Должен вернуться None
   - Должно быть залогировано ParseError

4. **Тест потоковой загрузки**
   - Проверить что используется stream=True
   - Память не должна расти пропорционально размеру файла
   - Чтение должно быть постепенным

### Потенциальные Проблемы

1. **Использование памяти**
   - f.read() читает весь файл в память после декомпрессии
   - Для огромных sitemap может быть проблема
   - Выигрыш stream=True теряется

2. **Повреждённые архивы**
   - Частично загруженные .gz файлы
   - Повреждение при передаче
   - gzip может упасть неожиданно

3. **Производительность**
   - Декомпрессия CPU-intensive
   - Может быть узким местом для больших файлов
   - Рассмотреть параллельную декомпрессию

4. **Различия с fetch_xml**
   - Две почти идентичные функции
   - Дублирование кода
   - Рефакторинг: общая функция с параметром

### Критерии Принятия

- [ ] Функция decompress_gz реализована
- [ ] Потоковая загрузка работает
- [ ] Декомпрессия через gzip.open
- [ ] Парсинг XML успешен
- [ ] Обработка ошибок декомпрессии
- [ ] Тесты с mock данными проходят

---

# МОДУЛЬ PARSER

## Задача 3.1: Создание Модуля Парсера

### Цель
Создать файл parser.py с необходимыми импортами и структурой.

### Детали Реализации

1. **Создание файла**
   - Файл: sitemap_extract/parser.py
   - Module docstring с описанием назначения

2. **Импорты**
   - from xml.etree import ElementTree as ET
   - from typing import List, Tuple
   - from .constants import XML_NAMESPACE
   - from .network import fetch_xml, decompress_gz
   - from .file_operations import save_urls (будет позже)
   - import logging для логирования

3. **Получение logger**
   - logger = logging.getLogger(__name__)
   - Использовать этот logger во всех функциях

### Критерии Принятия

- [ ] Файл parser.py создан
- [ ] Все необходимые импорты добавлены
- [ ] Logger инициализирован
- [ ] Docstring добавлен

---

## Задача 3.2: Извлечение Sitemap URLs

### Цель
Реализовать функцию extract_sitemap_urls для извлечения ссылок на вложенные sitemap.

### Детали Реализации

1. **Сигнатура функции**
   - Имя: extract_sitemap_urls
   - Параметр: root (ET.Element)
   - Возврат: List[str]

2. **Алгоритм**
   - Использовать XML_NAMESPACE из констант
   - XPath: root.findall('.//sm:sitemap', XML_NAMESPACE)
   - Для каждого элемента sitemap:
     - Найти дочерний sm:loc
     - Извлечь .text
     - Добавить в список
   - Вернуть список URL

3. **Обработка edge cases**
   - ЕСЛИ .findall возвращает пустой список: вернуть []
   - ЕСЛИ sm:loc отсутствует: пропустить элемент
   - ЕСЛИ .text пустой или None: пропустить

4. **Логирование**
   - DEBUG: количество найденных элементов sitemap
   - DEBUG: каждый извлеченный URL

### Сценарии Тестирования

1. **Тест с несколькими sitemap**
   - XML с 3 <sitemap> элементами
   - Должен вернуть список из 3 URL
   - URL должны совпадать с <loc> значениями

2. **Тест с пустым XML**
   - XML без <sitemap> элементов
   - Должен вернуть пустой список
   - Без ошибок

3. **Тест с отсутствующим <loc>**
   - <sitemap> без <loc> элемента
   - Должен пропустить этот sitemap
   - Вернуть только валидные URL

4. **Тест с пустым <loc>**
   - <sitemap><loc></loc></sitemap>
   - Должен пропустить или вернуть пустую строку
   - Определить желаемое поведение

### Потенциальные Проблемы

1. **Namespace проблемы**
   - Неправильный namespace URI
   - Отсутствие namespace в XML
   - Различные префиксы namespace

2. **XPath синтаксис**
   - './/sm:sitemap' vs '//sm:sitemap'
   - ElementTree ограниченная поддержка XPath
   - Может не работать с lxml

3. **Whitespace в URL**
   - .text может содержать whitespace
   - Применить .strip() к каждому URL
   - Валидация формата URL

### Критерии Принятия

- [ ] Функция extract_sitemap_urls реализована
- [ ] Использует правильный namespace
- [ ] Возвращает список URL
- [ ] Обрабатывает пустые результаты
- [ ] Тесты проходят

---

## Задача 3.3: Извлечение Page URLs

### Цель
Реализовать функцию extract_page_urls для извлечения ссылок на страницы сайта.

### Детали Реализации

1. **Сигнатура функции**
   - Имя: extract_page_urls
   - Параметр: root (ET.Element)
   - Возврат: List[str]

2. **Алгоритм**
   - Использовать тот же XML_NAMESPACE
   - XPath: root.findall('.//sm:url', XML_NAMESPACE)
   - Для каждого элемента url:
     - Найти дочерний sm:loc
     - Извлечь .text
     - Применить .strip()
     - Добавить в список
   - Вернуть список URL

3. **Игнорирование других элементов**
   - <lastmod>, <changefreq>, <priority> игнорируются
   - Только <loc> извлекается
   - Это соответствует спецификации

4. **Логирование**
   - DEBUG: количество найденных URL
   - DEBUG: первые N URL (для проверки)
   - INFO: итоговое количество извлеченных URL

### Сценарии Тестирования

1. **Тест с множественными URL**
   - XML с 10 <url> элементами
   - Должен вернуть 10 URL
   - Порядок сохраняется

2. **Тест игнорирования метаданных**
   - XML с <url> содержащими <lastmod> и др.
   - Должны быть извлечены только <loc>
   - Метаданные игнорируются

3. **Тест производительности**
   - XML с тысячами <url>
   - Должен обработаться за разумное время
   - Память не должна переполниться

### Потенциальные Проблемы

1. **Дублирование кода**
   - extract_sitemap_urls и extract_page_urls почти идентичны
   - Только XPath отличается
   - Рефакторинг: общая функция

2. **Большие списки**
   - Sitemap может содержать 50,000 URL
   - Список в памяти может быть большим
   - Рассмотреть генераторы

3. **Валидация URL**
   - Нет проверки что text это валидный URL
   - Могут быть относительные URL
   - Могут быть некорректные форматы

### Критерии Принятия

- [ ] Функция extract_page_urls реализована
- [ ] Извлекает только <loc> элементы
- [ ] Возвращает список URL
- [ ] Тесты с большими списками проходят

---

## Задача 3.4: Основная Функция Обработки Sitemap

### Цель
Реализовать process_sitemap - главную функцию для обработки одной sitemap.

### Детали Реализации

1. **Сигнатура**
   - Имя: process_sitemap
   - Параметры: url (str), is_compressed (bool), use_cloudscraper (bool), use_proxy (bool)
   - Возврат: Tuple[List[str], List[str]]

2. **Логика загрузки**
   - ЕСЛИ is_compressed == True: root = decompress_gz(url, ...)
   - ИНАЧЕ: root = fetch_xml(url, ...)
   - ЕСЛИ root is None: вернуть ([], [])

3. **Извлечение данных**
   - sitemap_urls = extract_sitemap_urls(root)
   - page_urls = extract_page_urls(root)

4. **Сохранение результатов**
   - ЕСЛИ page_urls не пуст:
     - Вызвать save_urls(url, page_urls)
   - Обработать возможные ошибки save_urls

5. **Возврат**
   - Вернуть кортеж (sitemap_urls, page_urls)

6. **Логирование**
   - INFO: начало обработки URL
   - DEBUG: is_compressed флаг
   - INFO: найдено X sitemap URLs, Y page URLs
   - ERROR: если загрузка упала

### Сценарии Тестирования

1. **Тест с обычным XML**
   - Mock fetch_xml с валидным XML
   - Должны извлечься sitemap и page URLs
   - save_urls должен быть вызван

2. **Тест с сжатым XML**
   - Mock decompress_gz с валидным XML
   - is_compressed=True
   - Должен использоваться decompress_gz

3. **Тест с ошибкой загрузки**
   - Mock fetch_xml возвращает None
   - Должен вернуться ([], [])
   - Нет краша

4. **Тест с пустым sitemap**
   - XML без <sitemap> и <url> элементов
   - Должен вернуться ([], [])
   - save_urls не должен вызываться

5. **Тест интеграции**
   - End-to-end с реальным (test) XML файлом
   - Проверка всех этапов
   - Проверка сохранённого файла

### Потенциальные Проблемы

1. **Зависимость от save_urls**
   - save_urls может упасть
   - Не должно ломать всю обработку
   - Try-except вокруг вызова

2. **Большие sitemap**
   - Обработка может занять время
   - Пользователь не видит прогресса
   - Рассмотреть progress logging

3. **Сетевые таймауты**
   - fetch_xml/decompress_gz могут зависнуть
   - Нет явного таймаута
   - Может блокировать поток надолго

### Критерии Принятия

- [ ] Функция process_sitemap реализована
- [ ] Выбор fetch_xml/decompress_gz работает
- [ ] Извлечение URL работает
- [ ] save_urls вызывается при наличии данных
- [ ] Возвращает правильный кортеж
- [ ] Обработка ошибок
- [ ] Тесты проходят

---

# МОДУЛЬ FILE OPERATIONS

## Задача 4.1: Создание Модуля

### Цель
Создать файл file_operations.py с базовой структурой.

### Детали Реализации

1. **Создание файла**
   - sitemap_extract/file_operations.py
   - Module docstring

2. **Импорты**
   - import os
   - import glob as glob_module
   - import logging
   - from typing import List

3. **Logger**
   - logger = logging.getLogger(__name__)

### Критерии Принятия

- [ ] Файл создан
- [ ] Импорты добавлены
- [ ] Logger инициализирован

---

## Задача 4.2: Генерация Имени Файла

### Цель
Реализовать функцию generate_filename для создания имени выходного файла из URL.

### Детали Реализации

1. **Сигнатура**
   - Имя: generate_filename
   - Параметр: url (str)
   - Возврат: str

2. **Алгоритм**
   - Разбить url.split('/')
   - Взять последний элемент [-1]
   - Разбить по '.' .split('.')
   - Взять первый элемент [0]
   - Добавить '.txt'
   - Вернуть результат

3. **Обработка edge cases**
   - URL заканчивается на '/' - использовать дефолтное имя
   - URL без '/' - использовать весь URL
   - Специальные символы в имени - очистить или escape

4. **Документация**
   - Предупреждение о возможных коллизиях имён
   - Примеры: "sitemap.xml" -> "sitemap.txt"

### Сценарии Тестирования

1. **Тест с обычным URL**
   - "https://example.com/sitemap.xml"
   - Ожидается: "sitemap.txt"

2. **Тест с .gz файлом**
   - "https://example.com/sitemap.xml.gz"
   - Ожидается: "sitemap.txt" (не "sitemap.xml.txt")

3. **Тест с URL без расширения**
   - "https://example.com/sitemap"
   - Ожидается: "sitemap.txt"

4. **Тест с trailing slash**
   - "https://example.com/sitemap/"
   - Определить поведение

5. **Тест коллизий**
   - Два URL с одинаковым именем файла
   - Второй перезапишет первый
   - Документировать это поведение

### Потенциальные Проблемы

1. **Коллизии имён**
   - Разные URL могут дать одинаковое имя
   - Перезапись файлов
   - Рекомендация: добавить timestamp или hash

2. **Недопустимые символы**
   - Некоторые символы недопустимы в именах файлов
   - Различия между ОС (Windows vs Linux)
   - Нужна санитизация

3. **Длина имени**
   - Очень длинные имена файлов
   - Ограничения файловой системы
   - Truncate если необходимо

### Критерии Принятия

- [ ] Функция generate_filename реализована
- [ ] Алгоритм работает корректно
- [ ] Тесты с различными URL проходят
- [ ] Коллизии документированы

---

## Задача 4.3: Сохранение URL в Файл

### Цель
Реализовать функцию save_urls для сохранения списка URL в текстовый файл.

### Детали Реализации

1. **Сигнатура**
   - Имя: save_urls
   - Параметры: url (str), urls (List[str])
   - Возврат: None

2. **Алгоритм**
   - filename = generate_filename(url)
   - Открыть файл в режиме 'w' (перезапись)
   - Записать "Source URL: {url}\n"
   - Для каждого URL в urls:
     - Записать "{url}\n"
   - Закрыть файл

3. **Обработка ошибок**
   - Try-except вокруг всего
   - Ловить IOError, PermissionError
   - Логировать ERROR с деталями
   - Не поднимать исключение (graceful)

4. **Логирование**
   - INFO: "Saved {count} URLs to {filename}"
   - ERROR: детали при ошибке

### Сценарии Тестирования

1. **Тест успешного сохранения**
   - Список из 10 URL
   - Файл должен быть создан
   - Содержимое должно совпадать

2. **Тест формата файла**
   - Первая строка: "Source URL: ..."
   - Остальные строки: по одному URL
   - Нет пустых строк в конце

3. **Тест с пустым списком**
   - urls = []
   - Файл должен содержать только Source URL
   - Нет ошибок

4. **Тест с большим списком**
   - 50,000 URL
   - Файл создаётся
   - Производительность приемлема

5. **Тест ошибок записи**
   - Нет прав на запись
   - Диск полон
   - Должна быть залогирована ошибка

### Потенциальные Проблемы

1. **Кодировка**
   - URL могут содержать non-ASCII символы
   - Явно указать encoding='utf-8'
   - Проблемы на Windows с default кодировкой

2. **Атомарность**
   - Файл может быть частично записан при краше
   - Временный файл + rename для атомарности
   - Избыточно для данного use case

3. **Параллелизм**
   - Множественные потоки пишут в разные файлы - OK
   - Если в один файл - конфликт
   - Текущий дизайн исключает это

### Критерии Принятия

- [ ] Функция save_urls реализована
- [ ] Файл создаётся с правильным именем
- [ ] Формат соответствует спецификации
- [ ] Ошибки обрабатываются gracefully
- [ ] Логирование работает
- [ ] Тесты проходят

---

## Задача 4.4: Чтение URL из Файла

### Цель
Реализовать функцию read_urls_from_file для чтения списка URL из текстового файла.

### Детали Реализации

1. **Сигнатура**
   - Имя: read_urls_from_file
   - Параметр: file_path (str)
   - Возврат: List[str]

2. **Алгоритм**
   - Открыть файл в режиме 'r'
   - Прочитать все строки (readlines() или итерация)
   - Для каждой строки:
     - Применить .strip()
     - Если не пустая: добавить в список
   - Вернуть список

3. **Обработка ошибок**
   - Try-except для FileNotFoundError
   - Try-except для PermissionError
   - Логировать ошибку
   - Вернуть пустой список при ошибке

4. **Логирование**
   - DEBUG: "Reading URLs from {file_path}"
   - INFO: "Read {count} URLs from {file_path}"
   - ERROR: детали ошибки

### Сценарии Тестирования

1. **Тест с валидным файлом**
   - Файл с 5 URL, по одному на строку
   - Должен вернуться список из 5 элементов

2. **Тест с пустыми строками**
   - Файл с URL и пустыми строками
   - Пустые строки должны быть отфильтрованы

3. **Тест с whitespace**
   - URL с ведущими/завершающими пробелами
   - Должны быть очищены .strip()

4. **Тест с несуществующим файлом**
   - file_path не существует
   - Должен вернуться []
   - Ошибка залогирована

5. **Тест с недоступным файлом**
   - Файл существует но нет прав на чтение
   - Должен вернуться []
   - PermissionError залогирован

### Потенциальные Проблемы

1. **Формат файла**
   - Предполагается один URL на строку
   - Что если URL многострочный?
   - Что если файл содержит комментарии?

2. **Кодировка**
   - Файл может быть в различных кодировках
   - Явно указать encoding='utf-8'
   - Обработать UnicodeDecodeError

3. **Большие файлы**
   - Файл с тысячами URL
   - readlines() загружает все в память
   - Для очень больших файлов использовать итерацию

### Критерии Принятия

- [ ] Функция read_urls_from_file реализована
- [ ] Читает URL построчно
- [ ] Очищает whitespace
- [ ] Фильтрует пустые строки
- [ ] Обрабатывает ошибки
- [ ] Тесты проходят

---

## Задача 4.5: Поиск XML Файлов

### Цель
Реализовать функцию find_xml_files_in_directory для поиска всех XML и XML.GZ файлов в директории.

### Детали Реализации

1. **Сигнатура**
   - Имя: find_xml_files_in_directory
   - Параметр: directory (str)
   - Возврат: List[str]

2. **Алгоритм**
   - xml_files = glob.glob(os.path.join(directory, '*.xml'))
   - gz_files = glob.glob(os.path.join(directory, '*.xml.gz'))
   - Объединить оба списка
   - Вернуть объединённый список

3. **Обработка ошибок**
   - Try-except для OSError
   - Логировать ошибки
   - Вернуть [] при ошибке

4. **Валидация**
   - Проверить что directory существует
   - Проверить что это директория (не файл)

5. **Логирование**
   - DEBUG: "Scanning {directory} for XML files"
   - INFO: "Found {count} XML files"

### Сценарии Тестирования

1. **Тест с XML файлами**
   - Директория с 3 .xml файлами
   - Должен вернуть 3 пути

2. **Тест с .gz файлами**
   - Директория с 2 .xml.gz файла
   - Должен вернуть 2 пути

3. **Тест со смешанными файлами**
   - Директория с .xml, .xml.gz, .txt файлами
   - Должны вернуться только XML и GZ

4. **Тест с пустой директорией**
   - Директория без XML файлов
   - Должен вернуться []

5. **Тест с несуществующей директорией**
   - directory не существует
   - Должен вернуться []
   - Ошибка залогирована

### Потенциальные Проблемы

1. **Рекурсивность**
   - Текущая реализация не рекурсивная
   - Только top-level файлы
   - Рассмотреть ** pattern для рекурсии

2. **Сортировка**
   - Порядок файлов не определён
   - Может отличаться между запусками
   - Рассмотреть сортировку результата

3. **Символические ссылки**
   - glob следует symlinks по умолчанию
   - Может привести к циклам
   - Опция recursive=False помогает

4. **Проблема с file:// URL**
   - find_xml_files_in_directory возвращает пути
   - fetch_xml ожидает HTTP URL
   - Несовместимость (известная проблема из спецификаций)

### Критерии Принятия

- [ ] Функция find_xml_files_in_directory реализована
- [ ] Находит .xml файлы
- [ ] Находит .xml.gz файлы
- [ ] Возвращает полные пути
- [ ] Обрабатывает ошибки
- [ ] Тесты проходят

---

## Сводка Фазы 2

### Что Достигнуто

После завершения Фазы 2:

1. **Network модуль**
   - HTTP клиент с Cloudscraper
   - User-Agent ротация
   - Загрузка XML
   - Декомпрессия GZ

2. **Parser модуль**
   - Извлечение sitemap URLs
   - Извлечение page URLs
   - Главная функция обработки

3. **File Operations модуль**
   - Генерация имён файлов
   - Сохранение URL
   - Чтение URL из файлов
   - Поиск XML файлов

### Критические Проблемы для Внимания

1. **Хардкоженный прокси** - требует конфигурации
2. **Устаревшие User-Agents** - обновить список
3. **Коллизии имён файлов** - рассмотреть timestamp
4. **Обработка директорий** - file:// vs HTTP URL несовместимость
5. **Отсутствие таймаутов** - добавить timeout в requests
6. **Ротация логов** - не реализована

### Следующая Фаза

Фаза 3 объединит эти модули через Orchestrator и добавит CLI интерфейс.
